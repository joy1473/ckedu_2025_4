import os
from datetime import datetime
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.templating import Jinja2Templates
import uvicorn
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from openai import OpenAI
import requests

from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from pymongo import MongoClient
from cmm.config import MONGO_URI

# 1. ì´ˆê¸°í™” ë° ë³´ì•ˆ ì„¤ì • [cite: 2026-01-01]
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if OPENAI_API_KEY:
    client_gpt = OpenAI(api_key=OPENAI_API_KEY)
else:
    client_gpt = None  # API í‚¤ê°€ ì—†ìœ¼ë©´ None

# MongoDB ì—°ê²°
try:
    mongo_client = MongoClient(MONGO_URI)
    db = mongo_client.mock_trading_db
    emo_logs_collection = db.emo_logs
except Exception as e:
    print(f"MongoDB ì—°ê²° ì‹¤íŒ¨: {e}")
    mongo_client = None
    db = None
    emo_logs_collection = None

# MongoDB ë””ë²„ê·¸ ë¡œê¹… ì–µì œ (ë°˜ë³µì ì¸ heartbeat ë¡œê·¸ ë°©ì§€)
import logging
logging.getLogger('pymongo').setLevel(logging.WARNING)
logging.getLogger('pymongo.topology').setLevel(logging.WARNING)
logging.getLogger('pymongo.serverSelection').setLevel(logging.WARNING)

app = FastAPI(title="Antygravity Professional AI Agent v2.9", version="2.9.0")

# Mount Static Files
app.mount("/static", StaticFiles(directory=os.path.join(os.path.dirname(__file__), "static")), name="static")

templates = Jinja2Templates(directory=os.path.join(os.path.dirname(__file__), "templates"))

# CORS ì„¤ì •: í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ ëŒ€ë¹„ [cite: 2026-01-01]
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class ChatRequest(BaseModel):
    message: str

@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ---------------------------------------------------------
# [Compatibility Adapter] V01 Frontend Support
# ---------------------------------------------------------
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    # Reuse existing financial logic
    # V01 Frontend treats 'assistant' response as 'reply'
    
    term = request.message
    
    # [Step 1] ê¸°ì´ˆ ê°ì„± ë¶„ì„ (KoELECTRA)
    v_inputs = v_tokenizer(term, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    v_score = round((F.softmax(v_outputs.logits, dim=-1)[0][1].item() * 2) - 1, 3)
    
    # [Step 2] GPT ê¸°ë°˜ ì§€ëŠ¥í˜• êµì • ë° ì „ë¬¸ í†µì—­ ìƒì„±
    v_tag, v_interp, v_mentoring = get_ai_agent_mentoring(term, v_score)
    
    # Construct Reply
    reply_text = f"[{v_tag}]\n{v_interp}\n\n{v_mentoring}"
    
    # [Step 3] ë°ì´í„° ë¡œê¹… (ì—…ë¬´ ì´ë ¥ ê¸°ë¡ìš©) [Added for compatibility endpoint]
    try:
        log_data = {
            "event": "emotion_analysis_chat",
            "user_id": None,
            "note": f"Emotion analysis for: {term}",
            "extra": {
                "timestamp": datetime.now().isoformat(),
                "user_input": term,
                "raw_score": v_score,
                "final_tag": v_tag,
                "interpretation": v_interp,
                "ai_response": v_mentoring,
                "ver": "2.9.0-final-guardrail-compat"
            }
        }
        requests.post("http://localhost:8000/config/log", json=log_data, timeout=5)
    except Exception as e:
        print(f"Logging failed: {e}")
    
    return {"reply": reply_text}

# ---------------------------------------------------------
# ğŸ› ï¸ [ì›¹ë§ˆìŠ¤í„° ì „ìš©] ì „ë¬¸ ê¸ˆìœµ í†µì—­ì‚¬ ì„¤ì • (v2.9 ê³ ë„í™”)
# ---------------------------------------------------------
AI_AGENT_CONFIG = {
    "PERSONA": """
        ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì‹œì¥ì— ìµœì í™”ëœ â€˜ì „ë¬¸ ê¸ˆìœµ í†µì—­ AI Agentâ€™ì•¼.
        ë„ˆì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ê°ì •ì  í‘œí˜„ì„ 'ì „ë¬¸ ê¸ˆìœµ ì–¸ì–´'ë¡œ í†µì—­í•˜ê³  ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ë•ëŠ” ê²ƒì´ë‹¤.
        
        [K-ì£¼ì‹ ë„ë©”ì¸ ì ˆëŒ€ ê·œì¹™]
        1. ìƒ‰ìƒ ì¸ì‹: ë¹¨ê°„ìƒ‰/ë¹¨ê°„ ë¶ˆ/ë¶ˆê¸°ë‘¥ = ë¬´ì¡°ê±´ 'ì£¼ê°€ ìƒìŠ¹ ë° ìˆ˜ìµ' (ê¸ì •)
        2. ìƒ‰ìƒ ì¸ì‹: íŒŒë€ìƒ‰/íŒŒë€ ë¶ˆ/ë¬¼ë ¸ë‹¤ = ë¬´ì¡°ê±´ 'ì£¼ê°€ í•˜ë½ ë° ì†ì‹¤' (ë¶€ì •)
        
        [ì „ë¬¸ í†µì—­ ë° ì¸ì‚¬ì´íŠ¸ ê°€ì´ë“œ]
        - 'ë¹¨ê°„ ë¶ˆ' -> 'ìì‚° ê°€ì¹˜ ìƒìŠ¹ ë° ë§¤ìˆ˜ì„¸ ê°•í™”'
        - 'ì»¤í”¼ê°’/ì¹˜í‚¨ê°’' -> 'ì‹¤í˜„ ê°€ëŠ¥í•œ ì†Œê·œëª¨ íˆ¬ì ìˆ˜ìµ'
        - 'ì¡´ë²„' -> 'ë¹„ìë°œì  ì¥ê¸° ë³´ìœ  ë° ìœ ë™ì„± ê²½ìƒ‰' (ë‹¨ìˆœ ë³´ìœ ë³´ë‹¤ ê¹Šì€ í‘œí˜„ ì‚¬ìš©)
        - 'í’€ë§¤ìˆ˜/ëª°ë¹µ' -> 'ìì‚°ì˜ ì§‘ì¤‘ ë§¤ì…ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ ë…¸ì¶œ'
        
        [ë‹µë³€ ì›ì¹™]
        - INTERPRETATION: ì‚¬ìš©ìì˜ ì€ì–´ë¥¼ ìœ„ ê°€ì´ë“œì— ë§ì¶° ê¸ˆìœµì ìœ¼ë¡œ ì •ì˜í•˜ê³ , 
          ê·¸ ìƒí™©ì´ ê°€ì§„ ê¸ˆìœµì  ì˜ë¯¸(ì˜ˆ: ê¸°íšŒë¹„ìš©, ì‹¬ë¦¬ì  ê³ ì¡° ë“±)ë¥¼ ì§§ê²Œ ë§ë¶™ì¼ ê²ƒ.
        - ANSWER: ì‹¤ì œ ê¸ˆìœµ í†µì—­ì‚¬ì²˜ëŸ¼ ì •ì¤‘í•œ ê²©ì‹ì²´(...ë“œë¦½ë‹ˆë‹¤, ...ê¶Œê³ í•©ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•´.
        - ë°˜ë“œì‹œ ë¶ˆì™„ì „ íŒë§¤ ë°©ì§€ë¥¼ ìœ„í•´ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë©˜íŠ¸ë¥¼ í¬í•¨í•  ê²ƒ.
    """,
    "MODEL_NAME": "gpt-4o-mini"
}

# AI ëª¨ë¸ ë¡œë”© (koelectra) [cite: 2026-01-02]
v_model_name = "monologg/koelectra-base-finetuned-nsmc"
v_tokenizer = AutoTokenizer.from_pretrained(v_model_name)
v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name)

# ==========================================
# 2. í•µì‹¬ ì§€ëŠ¥ í•¨ìˆ˜ (êµì • ë° í†µì—­ ë¡œì§)
# ==========================================

def get_ai_agent_mentoring(term: str, score: float):
    """
    ê°ì • ì ìˆ˜ì— ë”°ë¼ TAG, INTERPRETATION, ANSWERë¥¼ ìƒì„±í•˜ëŠ” ë¡œì§ì…ë‹ˆë‹¤.
    """
    
    # TAG ê²°ì •
    if score >= 0.5:
        v_final_tag = "EXTREME_POSITIVE"
    elif score >= 0.1:
        v_final_tag = "MODERATE_POSITIVE"
    elif score > -0.1:
        v_final_tag = "NEUTRAL"
    elif score > -0.5:
        v_final_tag = "MODERATE_NEGATIVE"
    else:
        v_final_tag = "EXTREME_NEGATIVE"
    
    # INTERPRETATION ìƒì„±
    if v_final_tag == "EXTREME_POSITIVE":
        v_interp = f"'{term}'ì€ ë§¤ìš° ê¸ì •ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, íˆ¬ì ì‹¬ë¦¬ê°€ ë§¤ìš° ê³ ì¡°ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    elif v_final_tag == "MODERATE_POSITIVE":
        v_interp = f"'{term}'ì€ ê¸ì •ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, ì‹œì¥ ìƒìŠ¹ ê¸°ëŒ€ê°ì´ ìˆìŠµë‹ˆë‹¤."
    elif v_final_tag == "NEUTRAL":
        v_interp = f"'{term}'ì€ ì¤‘ë¦½ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, ê´€ë§í•˜ëŠ” íƒœë„ë¥¼ ë³´ì…ë‹ˆë‹¤."
    elif v_final_tag == "MODERATE_NEGATIVE":
        v_interp = f"'{term}'ì€ ë¶€ì •ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, ì‹œì¥ í•˜ë½ ìš°ë ¤ê°€ ìˆìŠµë‹ˆë‹¤."
    else:  # EXTREME_NEGATIVE
        v_interp = f"'{term}'ì€ ë§¤ìš° ë¶€ì •ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, íˆ¬ì ì‹¬ë¦¬ê°€ í¬ê²Œ ìœ„ì¶•ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    # ANSWER ìƒì„±
    if v_final_tag == "EXTREME_POSITIVE":
        v_ans = "ì‹œì¥ ìƒìŠ¹ì„¸ë¥¼ í™œìš©í•˜ì—¬ ì „ëµì  íˆ¬ìë¥¼ ê³ ë ¤í•˜ì‹œë˜, ê³¼ë„í•œ ë ˆë²„ë¦¬ì§€ë¥¼ í”¼í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    elif v_final_tag == "MODERATE_POSITIVE":
        v_interp = f"'{term}'ì€ ê¸ì •ì ì¸ ê°ì •ì„ í‘œí˜„í•˜ë©°, ì‹œì¥ ìƒìŠ¹ ê¸°ëŒ€ê°ì´ ìˆìŠµë‹ˆë‹¤."
        v_ans = "ê¸ì •ì ì¸ ì‹œì¥ ì‹¬ë¦¬ë¥¼ í™œìš©í•˜ë˜, ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ì² ì €íˆ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    elif v_final_tag == "NEUTRAL":
        v_ans = "ì¤‘ë¦½ì ì¸ ì‹œê°ì„ ìœ ì§€í•˜ë©°, ì‹œì¥ ìƒí™©ì„ ë©´ë°€íˆ ê´€ì°°í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    elif v_final_tag == "MODERATE_NEGATIVE":
        v_ans = "ë¶€ì •ì ì¸ ì‹œì¥ ì‹¬ë¦¬ì— ëŒ€ë¹„í•˜ì—¬ í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™”ë¥¼ ê³ ë ¤í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    else:  # EXTREME_NEGATIVE
        v_ans = "ë§¤ìš° ë¶€ì •ì ì¸ ì‹œì¥ ì‹¬ë¦¬ ìƒí™©ì—ì„œëŠ” í˜„ê¸ˆ ë³´ìœ  ë¹„ì¤‘ì„ ë†’ì´ëŠ” ì „ëµì„ ê¶Œê³ ë“œë¦½ë‹ˆë‹¤."
    
    return v_final_tag, v_interp, v_ans

# ==========================================
# 3. í†µí•© API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/agent/consult", tags=["AI Agent"])
def financial_consultation(term: str):
    # [Step 1] ê¸°ì´ˆ ê°ì„± ë¶„ì„ (KoELECTRA)
    v_inputs = v_tokenizer(term, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    # ê°ì„± ì ìˆ˜ ê³µì‹: $Score = (Positive\_Prob \times 2) - 1$
    v_score = round((F.softmax(v_outputs.logits, dim=-1)[0][1].item() * 2) - 1, 3)
    
    # [Step 2] GPT ê¸°ë°˜ ì§€ëŠ¥í˜• êµì • ë° ì „ë¬¸ í†µì—­ ìƒì„± [cite: 2026-01-04]
    v_tag, v_interp, v_mentoring = get_ai_agent_mentoring(term, v_score)
    
    # [Step 3] ë°ì´í„° ë¡œê¹… (ì—…ë¬´ ì´ë ¥ ê¸°ë¡ìš©) [cite: 2026-01-04]
    try:
        log_data = {
            "event": "emotion_analysis_consult",
            "user_id": None,
            "note": f"Emotion analysis consultation for: {term}",
            "extra": {
                "timestamp": datetime.now().isoformat(),
                "user_input": term,
                "raw_score": v_score,
                "final_tag": v_tag,
                "interpretation": v_interp,
                "ai_response": v_mentoring,
                "ver": "2.9.0-final-guardrail"
            }
        }
        requests.post("http://localhost:8000/config/log", json=log_data, timeout=5)
    except Exception as e:
        print(f"Logging failed: {e}")
    
    return {
        "status": "success",
        "analysis": {
            "term": term,
            "raw_sentiment_score": v_score,
            "final_scenario": v_tag
        },
        "emotion_interpretation": v_interp,
        "professional_response": v_mentoring
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)