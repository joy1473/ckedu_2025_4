import os
from datetime import datetime
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pymongo import MongoClient
import uvicorn
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from openai import OpenAI

# 1. ì´ˆê¸°í™” ë° ë³´ì•ˆ ì„¤ì • [cite: 2026-01-01]
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client_gpt = OpenAI(api_key=OPENAI_API_KEY)

app = FastAPI(title="Antygravity Professional AI Agent v2.9", version="2.9.0")

# CORS ì„¤ì •: í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ ëŒ€ë¹„ [cite: 2026-01-01]
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ---------------------------------------------------------
# ğŸ› ï¸ [ì›¹ë§ˆìŠ¤í„° ì „ìš©] ì „ë¬¸ ê¸ˆìœµ í†µì—­ì‚¬ ì„¤ì • (v2.9 ê³ ë„í™”)
# ---------------------------------------------------------
AI_AGENT_CONFIG = {
    "PERSONA": """
        ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì‹œì¥ì— ìµœì í™”ëœ â€˜ì „ë¬¸ ê¸ˆìœµ í†µì—­ AI Agentâ€™ì•¼.
        ë„ˆì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ê°ì •ì  í‘œí˜„ì„ 'ì „ë¬¸ ê¸ˆìœµ ì–¸ì–´'ë¡œ í†µì—­í•˜ê³  ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ë•ëŠ” ê²ƒì´ë‹¤.
        
        [K-ì£¼ì‹ ë„ë©”ì¸ ì ˆëŒ€ ê·œì¹™]
        1. ìƒ‰ìƒ ì¸ì‹: ë¹¨ê°„ìƒ‰/ë¹¨ê°„ ë¶ˆ/ë¶ˆê¸°ë‘¥ = ë¬´ì¡°ê±´ 'ì£¼ê°€ ìƒìŠ¹ ë° ìˆ˜ìµ' (ê¸ì •)
        2. ìƒ‰ìƒ ì¸ì‹: íŒŒë€ìƒ‰/íŒŒë€ ë¶ˆ/ë¬¼ë ¸ë‹¤ = ë¬´ì¡°ê±´ 'ì£¼ê°€ í•˜ë½ ë° ì†ì‹¤' (ë¶€ì •)
        
        [ì „ë¬¸ í†µì—­ ë° ì¸ì‚¬ì´íŠ¸ ê°€ì´ë“œ]
        - 'ë¹¨ê°„ ë¶ˆ' -> 'ìì‚° ê°€ì¹˜ ìƒìŠ¹ ë° ë§¤ìˆ˜ì„¸ ê°•í™”'
        - 'ì»¤í”¼ê°’/ì¹˜í‚¨ê°’' -> 'ì‹¤í˜„ ê°€ëŠ¥í•œ ì†Œê·œëª¨ íˆ¬ì ìˆ˜ìµ'
        - 'ì¡´ë²„' -> 'ë¹„ìë°œì  ì¥ê¸° ë³´ìœ  ë° ìœ ë™ì„± ê²½ìƒ‰' (ë‹¨ìˆœ ë³´ìœ ë³´ë‹¤ ê¹Šì€ í‘œí˜„ ì‚¬ìš©)
        - 'í’€ë§¤ìˆ˜/ëª°ë¹µ' -> 'ìì‚°ì˜ ì§‘ì¤‘ ë§¤ì…ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ ë…¸ì¶œ'
        
        [ë‹µë³€ ì›ì¹™]
        - INTERPRETATION: ì‚¬ìš©ìì˜ ì€ì–´ë¥¼ ìœ„ ê°€ì´ë“œì— ë§ì¶° ê¸ˆìœµì ìœ¼ë¡œ ì •ì˜í•˜ê³ , 
          ê·¸ ìƒí™©ì´ ê°€ì§„ ê¸ˆìœµì  ì˜ë¯¸(ì˜ˆ: ê¸°íšŒë¹„ìš©, ì‹¬ë¦¬ì  ê³ ì¡° ë“±)ë¥¼ ì§§ê²Œ ë§ë¶™ì¼ ê²ƒ.
        - ANSWER: ì‹¤ì œ ê¸ˆìœµ í†µì—­ì‚¬ì²˜ëŸ¼ ì •ì¤‘í•œ ê²©ì‹ì²´(...ë“œë¦½ë‹ˆë‹¤, ...ê¶Œê³ í•©ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•´.
        - ë°˜ë“œì‹œ ë¶ˆì™„ì „ íŒë§¤ ë°©ì§€ë¥¼ ìœ„í•´ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë©˜íŠ¸ë¥¼ í¬í•¨í•  ê²ƒ.
    """,
    "MODEL_NAME": "gpt-4o-mini"
}

# AI ëª¨ë¸ ë¡œë”© (koelectra) [cite: 2026-01-02]
v_model_name = "monologg/koelectra-base-finetuned-nsmc"
v_tokenizer = AutoTokenizer.from_pretrained(v_model_name)
v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name)

# DB ì—°ê²° (MongoDB) [cite: 2026-01-01]
client_db = MongoClient("mongodb://localhost:27017/")
db = client_db["game_db"]
log_collection = db["test_logs"]

# ==========================================
# 2. í•µì‹¬ ì§€ëŠ¥ í•¨ìˆ˜ (êµì • ë° í†µì—­ ë¡œì§)
# ==========================================

def get_ai_agent_mentoring(term: str, score: float):
    """
    GPTê°€ ì›ë¬¸ì„ ë¶„ì„í•˜ì—¬ ëª¨ë¸ ì ìˆ˜ë¥¼ êµì •í•˜ê³ , 
    [ê¸ˆìœµ í†µì—­]ê³¼ [ì „ë¬¸ ì¡°ì–¸]ì„ ìƒì„±í•˜ëŠ” í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤. [cite: 2026-01-04]
    """
    v_system_content = f"""
    {AI_AGENT_CONFIG["PERSONA"]}
    
    [ë¶„ì„ ë¯¸ì…˜]
    - ì…ë ¥ê°’: "{term}" (ê¸°ì´ˆ AI ì ìˆ˜: {score})
    
    [ì¶œë ¥ í˜•ì‹ - ì—„ê²© ì¤€ìˆ˜]
    TAG: (EXTREME_NEGATIVE / MODERATE_NEGATIVE / NEUTRAL / MODERATE_POSITIVE / EXTREME_POSITIVE)
    INTERPRETATION: (ìƒí™©ì— ëŒ€í•œ ê¸ˆìœµ í‘œì¤€ì–´ í†µì—­ ë° ì˜ë¯¸ ë¶„ì„)
    ANSWER: (ê²©ì¡° ìˆëŠ” ì „ë¬¸ ì¡°ì–¸)
    """
    
    try:
        response = client_gpt.chat.completions.create(
            model=AI_AGENT_CONFIG["MODEL_NAME"],
            messages=[{"role": "system", "content": v_system_content}],
            temperature=0.1 # ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ê²Œ ê³ ì •
        )
        res_text = response.choices[0].message.content
        
        # í…ìŠ¤íŠ¸ íŒŒì‹± ì²˜ë¦¬
        v_final_tag, v_interp, v_ans = "NEUTRAL", "ë¶„ì„ ì‹¤íŒ¨", "ì¡°ì–¸ ìƒì„± ì¤‘ ì˜¤ë¥˜"
        lines = res_text.split('\n')
        for line in lines:
            if line.startswith("TAG:"): v_final_tag = line.replace("TAG:", "").strip()
            if line.startswith("INTERPRETATION:"): v_interp = line.replace("INTERPRETATION:", "").strip()
            if line.startswith("ANSWER:"): v_ans = line.replace("ANSWER:", "").strip()
            
        return v_final_tag, v_interp, v_ans
    except Exception as e:
        return "ERROR", f"í†µì—­ ì—”ì§„ ì˜¤ë¥˜: {str(e)}", "ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤."

# ==========================================
# 3. í†µí•© API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/agent/consult", tags=["AI Agent"])
def financial_consultation(term: str):
    # [Step 1] ê¸°ì´ˆ ê°ì„± ë¶„ì„ (KoELECTRA)
    v_inputs = v_tokenizer(term, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    # ê°ì„± ì ìˆ˜ ê³µì‹: $Score = (Positive\_Prob \times 2) - 1$
    v_score = round((F.softmax(v_outputs.logits, dim=-1)[0][1].item() * 2) - 1, 3)
    
    # [Step 2] GPT ê¸°ë°˜ ì§€ëŠ¥í˜• êµì • ë° ì „ë¬¸ í†µì—­ ìƒì„± [cite: 2026-01-04]
    v_tag, v_interp, v_mentoring = get_ai_agent_mentoring(term, v_score)
    
    # [Step 3] ë°ì´í„° ë¡œê¹… (ì—…ë¬´ ì´ë ¥ ê¸°ë¡ìš©) [cite: 2026-01-04]
    log_collection.insert_one({
        "timestamp": datetime.now(),
        "user_input": term,
        "raw_score": v_score,
        "final_tag": v_tag,
        "interpretation": v_interp,
        "ai_response": v_mentoring,
        "ver": "2.9.0-final-guardrail"
    })
    
    return {
        "status": "success",
        "analysis": {
            "term": term,
            "raw_sentiment_score": v_score,
            "final_scenario": v_tag
        },
        "emotion_interpretation": v_interp,
        "professional_response": v_mentoring
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)