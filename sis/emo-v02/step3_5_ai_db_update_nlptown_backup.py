# Backup of step3_5_ai_db_update.py (state when using nlptown fallback)
# Created automatically before attempting monologg/kcelectra access.
# You can restore this file if needed.

import datetime
import torch
from pymongo import MongoClient
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch.nn.functional as F

# ==========================================
# 1. AI ëª¨ë¸ ë¡œë“œ (KcELECTRA Finetuned)
# ==========================================
# ê¸°ë³¸: monologg/kcelectra-base-finetuned-nsmc (í•´ë‹¹ repoê°€ private/gatedì¼ ìˆ˜ ìˆìŒ)
# ë¡œë“œì— ì‹¤íŒ¨í•˜ë©´ ê³µê°œ sentiment ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.
from dotenv import load_dotenv
import os
load_dotenv()

# ìš°ì„ ìˆœìœ„: HUGGINGFACE_TOKEN ë˜ëŠ” HF_TOKEN í™˜ê²½ë³€ìˆ˜
hf_token = os.getenv("HUGGINGFACE_TOKEN") or os.getenv("HF_TOKEN")

v_model_name = "monologg/kcelectra-base-finetuned-nsmc"
try:
    if hf_token:
        print("ğŸ” Hugging Face token detected in environment; using authenticated download (masking token).")
        v_tokenizer = AutoTokenizer.from_pretrained(v_model_name, use_auth_token=hf_token)
        v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name, use_auth_token=hf_token)
    else:
        v_tokenizer = AutoTokenizer.from_pretrained(v_model_name)
        v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name)
    v_is_fallback_multiclass = False
except Exception as e:
    import sys
    print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    print("í•´ê²°ì±…: 1) ì´ ëª¨ë¸ì´ private/gatedì´ë©´ Hugging Face í† í°ì„ ë°œê¸‰í•´ì„œ .envì— `HUGGINGFACE_TOKEN=hf_xxx`ë¡œ ì„¤ì •í•˜ê±°ë‚˜, CLIë¡œ ë¡œê·¸ì¸í•˜ì„¸ìš”.")
    print("         2) CLI ë¡œê·¸ì¸: `huggingface-cli login` (ê¶Œí•œ ìˆëŠ” í† í°ìœ¼ë¡œ ë¡œê·¸ì¸)")
    print("         3) ë˜ëŠ” ê³µê°œ ëª¨ë¸ë¡œ í´ë°±í•©ë‹ˆë‹¤: nlptown/bert-base-multilingual-uncased-sentiment")

    # ê³µê°œ í´ë°± ëª¨ë¸ (ë‹¤ì¤‘ í´ë˜ìŠ¤: 1~5ì )
    v_model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
    try:
        if hf_token:
            v_tokenizer = AutoTokenizer.from_pretrained(v_model_name, use_auth_token=hf_token)
            v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name, use_auth_token=hf_token)
        else:
            v_tokenizer = AutoTokenizer.from_pretrained(v_model_name)
            v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name)
        v_is_fallback_multiclass = True
        print(f"í´ë°± ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {v_model_name}")
    except Exception as e2:
        print("âš ï¸ í´ë°± ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:", e2)
        print("ê°€ëŠ¥í•œ ì›ì¸: PyTorch/torchvisionì˜ ë²„ì „ ë¶ˆì¼ì¹˜ ë˜ëŠ” ì„¤ì¹˜ ë¬¸ì œ.")
        print("í•´ê²°: ì•„ë˜ ëª…ë ¹ì–´ë¡œ PyTorchì™€ torchvisionì„ í˜¸í™˜ ë²„ì „ìœ¼ë¡œ ì¬ì„¤ì¹˜í•˜ì„¸ìš” (ì˜ˆì‹œ):")
        print("  pip uninstall torchvision && pip install --upgrade --force-reinstall torch torchvision")
        print("ë˜ëŠ” í™˜ê²½ ì •ë³´(íŒŒì´ì¬, torch, torchvision ë²„ì „)ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.")
        sys.exit(1)

print(f"Loaded model: {v_model_name}")

# ==========================================
# 2. ê°ì„± ë¶„ì„ ì—”ì§„ í•¨ìˆ˜ (AI ë‡Œ)
# ==========================================

def get_ai_sentiment_score(in_text):
    v_inputs = v_tokenizer(in_text, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    v_probs = torch.nn.functional.softmax(v_outputs.logits, dim=-1)[0].cpu().numpy()

    if (not v_is_fallback_multiclass) and (v_probs.shape[0] == 2):
        v_pos_prob = float(v_probs[1])
        v_final_score = round((v_pos_prob * 2) - 1, 3)
    else:
        classes = v_probs.shape[0]
        expected_rating = sum((i + 1) * float(p) for i, p in enumerate(v_probs))
        mid = (classes + 1) / 2.0
        denom = (classes - 1) / 2.0
        v_final_score = round((expected_rating - mid) / denom, 3)

    return v_final_score

# (Rest of the original file continues...)