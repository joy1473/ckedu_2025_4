import os
from datetime import datetime
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.templating import Jinja2Templates
from pymongo import MongoClient
import uvicorn
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# 1. ì´ˆê¸°í™” ë° ë³´ì•ˆ ì„¤ì • [cite: 2026-01-01]
load_dotenv()

app = FastAPI(title="Antygravity Professional AI Agent v2.9", version="2.9.0")

# Mount Static Files
app.mount("/static", StaticFiles(directory="static"), name="static")

templates = Jinja2Templates(directory="templates")

# CORS ì„¤ì •: í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ ëŒ€ë¹„ [cite: 2026-01-01]
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

class ChatRequest(BaseModel):
    message: str

@app.get("/")
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

# ---------------------------------------------------------
# [Compatibility Adapter] V01 Frontend Support
# ---------------------------------------------------------
@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    # Reuse existing financial logic
    # V01 Frontend treats 'assistant' response as 'reply'
    
    term = request.message
    
    # [Step 1] ê¸°ì´ˆ ê°ì„± ë¶„ì„ (KoELECTRA)
    v_inputs = v_tokenizer(term, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    v_score = round((F.softmax(v_outputs.logits, dim=-1)[0][1].item() * 2) - 1, 3)
    
    # [Step 2] GPT ê¸°ë°˜ ì§€ëŠ¥í˜• êµì • ë° ì „ë¬¸ í†µì—­ ìƒì„±
    v_tag, v_interp, v_mentoring = get_ai_agent_mentoring(term, v_score)
    
    # Construct Reply
    reply_text = f"[{v_tag}]\n{v_interp}\n\n{v_mentoring}"
    
    # [Step 3] ë°ì´í„° ë¡œê¹… (ì—…ë¬´ ì´ë ¥ ê¸°ë¡ìš©) [Added for compatibility endpoint]
    log_collection.insert_one({
        "timestamp": datetime.now(),
        "user_input": term,
        "raw_score": v_score,
        "final_tag": v_tag,
        "interpretation": v_interp,
        "ai_response": v_mentoring,
        "ver": "2.9.0-final-guardrail-compat"
    })
    
    return {"reply": reply_text}

# ---------------------------------------------------------
# ðŸ› ï¸ [ì›¹ë§ˆìŠ¤í„° ì „ìš©] ì „ë¬¸ ê¸ˆìœµ í†µì—­ì‚¬ ì„¤ì • (v2.9 ê³ ë„í™”)
# ---------------------------------------------------------
AI_AGENT_CONFIG = {
    "PERSONA": """
        ë„ˆëŠ” í•œêµ­ ì£¼ì‹ ì‹œìž¥ì— ìµœì í™”ëœ â€˜ì „ë¬¸ ê¸ˆìœµ í†µì—­ AI Agentâ€™ì•¼.
        ë„ˆì˜ ëª©í‘œëŠ” ì‚¬ìš©ìžì˜ ê°ì •ì  í‘œí˜„ì„ 'ì „ë¬¸ ê¸ˆìœµ ì–¸ì–´'ë¡œ í†µì—­í•˜ê³  ë¦¬ìŠ¤í¬ ê´€ë¦¬ë¥¼ ë•ëŠ” ê²ƒì´ë‹¤.
        
        [K-ì£¼ì‹ ë„ë©”ì¸ ì ˆëŒ€ ê·œì¹™]
        1. ìƒ‰ìƒ ì¸ì‹: ë¹¨ê°„ìƒ‰/ë¹¨ê°„ ë¶ˆ/ë¶ˆê¸°ë‘¥ = ë¬´ì¡°ê±´ 'ì£¼ê°€ ìƒìŠ¹ ë° ìˆ˜ìµ' (ê¸ì •)
        2. ìƒ‰ìƒ ì¸ì‹: íŒŒëž€ìƒ‰/íŒŒëž€ ë¶ˆ/ë¬¼ë ¸ë‹¤ = ë¬´ì¡°ê±´ 'ì£¼ê°€ í•˜ë½ ë° ì†ì‹¤' (ë¶€ì •)
        
        [ì „ë¬¸ í†µì—­ ë° ì¸ì‚¬ì´íŠ¸ ê°€ì´ë“œ]
        - 'ë¹¨ê°„ ë¶ˆ' -> 'ìžì‚° ê°€ì¹˜ ìƒìŠ¹ ë° ë§¤ìˆ˜ì„¸ ê°•í™”'
        - 'ì»¤í”¼ê°’/ì¹˜í‚¨ê°’' -> 'ì‹¤í˜„ ê°€ëŠ¥í•œ ì†Œê·œëª¨ íˆ¬ìž ìˆ˜ìµ'
        - 'ì¡´ë²„' -> 'ë¹„ìžë°œì  ìž¥ê¸° ë³´ìœ  ë° ìœ ë™ì„± ê²½ìƒ‰' (ë‹¨ìˆœ ë³´ìœ ë³´ë‹¤ ê¹Šì€ í‘œí˜„ ì‚¬ìš©)
        - 'í’€ë§¤ìˆ˜/ëª°ë¹µ' -> 'ìžì‚°ì˜ ì§‘ì¤‘ ë§¤ìž…ì— ë”°ë¥¸ ë¦¬ìŠ¤í¬ ë…¸ì¶œ'
        
        [ë‹µë³€ ì›ì¹™]
        - INTERPRETATION: ì‚¬ìš©ìžì˜ ì€ì–´ë¥¼ ìœ„ ê°€ì´ë“œì— ë§žì¶° ê¸ˆìœµì ìœ¼ë¡œ ì •ì˜í•˜ê³ , 
          ê·¸ ìƒí™©ì´ ê°€ì§„ ê¸ˆìœµì  ì˜ë¯¸(ì˜ˆ: ê¸°íšŒë¹„ìš©, ì‹¬ë¦¬ì  ê³ ì¡° ë“±)ë¥¼ ì§§ê²Œ ë§ë¶™ì¼ ê²ƒ.
        - ANSWER: ì‹¤ì œ ê¸ˆìœµ í†µì—­ì‚¬ì²˜ëŸ¼ ì •ì¤‘í•œ ê²©ì‹ì²´(...ë“œë¦½ë‹ˆë‹¤, ...ê¶Œê³ í•©ë‹ˆë‹¤)ë¥¼ ì‚¬ìš©í•´.
        - ë°˜ë“œì‹œ ë¶ˆì™„ì „ íŒë§¤ ë°©ì§€ë¥¼ ìœ„í•´ ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë©˜íŠ¸ë¥¼ í¬í•¨í•  ê²ƒ.
    """,
    "MODEL_NAME": "gpt-4o-mini"
}

# AI ëª¨ë¸ ë¡œë”© (koelectra) [cite: 2026-01-02]
v_model_name = "monologg/koelectra-base-finetuned-nsmc"
v_tokenizer = AutoTokenizer.from_pretrained(v_model_name)
v_model = AutoModelForSequenceClassification.from_pretrained(v_model_name)

# DB ì—°ê²° (MongoDB) [cite: 2026-01-01]
client_db = MongoClient(os.getenv("MONGO_DB_URL"))
db = client_db["mock_trading_db"]
log_collection = db["emo_logs"]

# ==========================================
# 2. í•µì‹¬ ì§€ëŠ¥ í•¨ìˆ˜ (êµì • ë° í†µì—­ ë¡œì§)
# ==========================================

def get_ai_agent_mentoring(term: str, score: float):
    """
    GPTê°€ ì›ë¬¸ì„ ë¶„ì„í•˜ì—¬ ëª¨ë¸ ì ìˆ˜ë¥¼ êµì •í•˜ê³ , 
    [ê¸ˆìœµ í†µì—­]ê³¼ [ì „ë¬¸ ì¡°ì–¸]ì„ ìƒì„±í•˜ëŠ” í•µì‹¬ ë¡œì§ìž…ë‹ˆë‹¤. [cite: 2026-01-04]
    """
    # ê·œì¹™ ê¸°ë°˜ íƒœê¹… ë° ì „ë¬¸ ì¡°ì–¸ ìƒì„± (OpenAI ë¯¸ì‚¬ìš©)
    if score >= 0.6:
        v_final_tag = "EXTREME_POSITIVE"
        tag_kr = "ê°•ë ¥ ë§¤ìˆ˜ ìš°ìœ„"
    elif 0.2 <= score < 0.6:
        v_final_tag = "MODERATE_POSITIVE"
        tag_kr = "ë§¤ìˆ˜ ì‹¬ë¦¬ ìš°ì„¸"
    elif -0.2 < score < 0.2:
        v_final_tag = "NEUTRAL"
        tag_kr = "ê´€ë§ì„¸/ì¤‘ë¦½"
    elif -0.6 < score <= -0.2:
        v_final_tag = "MODERATE_NEGATIVE"
        tag_kr = "ë§¤ë„ ì••ë ¥ ê°ì§€"
    else:
        v_final_tag = "EXTREME_NEGATIVE"
        tag_kr = "ê³µí¬/íŒ¨ë‹‰ ì…€ë§"

    v_interp = f"ìž…ë ¥í•˜ì‹  '{term}'ì—ì„œ {tag_kr} ì‹œê·¸ë„({v_final_tag})ì´ í¬ì°©ë˜ì—ˆìŠµë‹ˆë‹¤. (AI ì ìˆ˜: {score})"

    if v_final_tag == "EXTREME_POSITIVE":
        v_ans = "ì‹œìž¥ì˜ ê¸ì •ì  ì—ë„ˆì§€ê°€ ìµœê³ ì¡°ìž…ë‹ˆë‹¤. ë‹¤ë§Œ, ê³¼ì—´ê¶Œ ì§„ìž… ê°€ëŠ¥ì„±ì„ ì—¼ë‘ì— ë‘ê³  ì°¨ìµ ì‹¤í˜„ ê³„íšì„ ì ê²€í•˜ì‹­ì‹œì˜¤."
    elif v_final_tag == "MODERATE_POSITIVE":
        v_ans = "ì–‘í˜¸í•œ ìƒìŠ¹ íë¦„ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤. íŽ€ë”ë©˜í„¸ì„ ì ê²€í•˜ë©° ì¶”ì„¸ ì¶”ì¢… ì „ëžµì„ ìœ ì§€í•˜ëŠ” ê²ƒì„ ê¶Œìž¥í•©ë‹ˆë‹¤."
    elif v_final_tag == "NEUTRAL":
        v_ans = "ë°©í–¥ì„±ì´ ëšœë ·í•˜ì§€ ì•Šì€ êµ¬ê°„ìž…ë‹ˆë‹¤. ì„£ë¶€ë¥¸ ì§„ìž…ë³´ë‹¤ëŠ” í˜„ê¸ˆì„ ë³´ìœ í•˜ë©° ë³€ë™ì„± í™•ëŒ€ë¥¼ ëŒ€ë¹„í•˜ì‹­ì‹œì˜¤."
    elif v_final_tag == "MODERATE_NEGATIVE":
        v_ans = "ì‹œìž¥ ì‹¬ë¦¬ê°€ ìœ„ì¶•ë˜ê³  ìžˆìŠµë‹ˆë‹¤. ê³µê²©ì ì¸ íˆ¬ìžëŠ” ì§€ì–‘í•˜ê³  ë¦¬ìŠ¤í¬ ê´€ë¦¬ì— ì§‘ì¤‘í•  ì‹œì ìž…ë‹ˆë‹¤."
    else:
        v_ans = "ê·¹ë„ì˜ ê³µí¬ ì‹¬ë¦¬ê°€ ì§€ë°°ì ìž…ë‹ˆë‹¤. íˆ¬ë§¤ì— ë™ì°¸í•˜ê¸°ë³´ë‹¤ ëƒ‰ì² í•˜ê²Œ ì‹œìž¥ì„ ì£¼ì‹œí•˜ë©° ìžì‚° ë°©ì–´ì— íž˜ì“°ì‹­ì‹œì˜¤."

    return v_final_tag, v_interp, v_ans

# ==========================================
# 3. í†µí•© API ì—”ë“œí¬ì¸íŠ¸
# ==========================================

@app.get("/agent/consult", tags=["AI Agent"])
def financial_consultation(term: str):
    # [Step 1] ê¸°ì´ˆ ê°ì„± ë¶„ì„ (KoELECTRA)
    v_inputs = v_tokenizer(term, return_tensors="pt", truncation=True, max_length=128)
    with torch.no_grad():
        v_outputs = v_model(**v_inputs)
    # ê°ì„± ì ìˆ˜ ê³µì‹: $Score = (Positive\_Prob \times 2) - 1$
    v_score = round((F.softmax(v_outputs.logits, dim=-1)[0][1].item() * 2) - 1, 3)
    
    # [Step 2] GPT ê¸°ë°˜ ì§€ëŠ¥í˜• êµì • ë° ì „ë¬¸ í†µì—­ ìƒì„± [cite: 2026-01-04]
    v_tag, v_interp, v_mentoring = get_ai_agent_mentoring(term, v_score)
    
    # [Step 3] ë°ì´í„° ë¡œê¹… (ì—…ë¬´ ì´ë ¥ ê¸°ë¡ìš©) [cite: 2026-01-04]
    log_collection.insert_one({
        "timestamp": datetime.now(),
        "user_input": term,
        "raw_score": v_score,
        "final_tag": v_tag,
        "interpretation": v_interp,
        "ai_response": v_mentoring,
        "ver": "2.9.0-final-guardrail"
    })
    
    return {
        "status": "success",
        "analysis": {
            "term": term,
            "raw_sentiment_score": v_score,
            "final_scenario": v_tag
        },
        "emotion_interpretation": v_interp,
        "professional_response": v_mentoring
    }

if __name__ == "__main__":
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)